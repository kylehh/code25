---
title: DPO - Direct Preference Optimization
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

Continue with another [blog](https://cameronrwolfe.substack.com/p/direct-preference-optimization) from Cameron and greate explanation and super easy to digest math details

## 1 Preference dataset 
PD of prompts with an associated “chosen” and “rejected” response.
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/pd.png)
and it can be integrated in to Bradley-Terry model
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/bt.png)

## 2 DPO in LLM Training
The DPO is replacing RLHF in the LLM training workflow
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/dpo_llm.png)
The standard RFHF has a reward model within its objective function
1. It uses explicit reward model
2. Use PPO to train RL
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/rlhf_loss.png)
But with DPO, we use 
1. implicit reward model within the policy itsel
2. indirectly derives the optimal policy from 
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/dpo_paper.png)

Here is the loss function for DPO
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/dpo_loss.png)
This is actually very close to the loss function we used to train reward model

## 3 Derive of DPO
1. Deriving an expression for the optimal policy in RLHF.
This step defines a **partition function** 
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/pf.png)
and defines **optimal policy**as below
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/op.png)
you can see that
- The value of the optimal policy is ≥ 0 for all possible completions y.
- The sum of the optimal policy across all completions y is equal to 1.
Further derivation shows that the policy being optimal policy can minimized the RLHF objective
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/klmini.png)
2. Deriving an implicit reward. 
Re-arrange the defination of optimal policy, we can get the implicti reward function
3. Plug into Bradley-Terry modal
4. Training an LLM to match this implicit preference model—this is what we are doing in the DPO training process.
![Alt text](/code23/assets/images/2025/25-07-31-DPO_files/dpo4.png)
